{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "207fe959",
   "metadata": {},
   "source": [
    "### Installing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf889b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7480b8",
   "metadata": {},
   "source": [
    "### Importing libraries required for web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d4c6b",
   "metadata": {},
   "source": [
    "### Creating a List of Companies for news article scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630baf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_companies=['reliance-industries','hdfc','ongc','indian-oil-corporation','adani-group',\n",
    "                     'hero-motocorp','asian-paints','eicher-motors','itc','tata-steel',\n",
    "                  'shriram-transport-finance','dr-reddys-laboratories','infosys','sun-pharma','tata-consultancy-services-(tcs)',\n",
    "                      'maruti-suzuki','hcl-technologies','coal-india','lti-mindtree','hdfc-life',\n",
    "                      'bajaj-auto','britannia-industries','hindalco-industries','larsen-and-toubro','tata-consumer-products',\n",
    "                      'wipro','titan','bajaj-finance','jsw-steel','icici-bank',\n",
    "                      'indusind-bank','bharti-airtel','divis-laboratories','sbi-life-insurance','bajaj-finserv',\n",
    "                      'cipla','grasim-industries','hindustan-unilever','mahindra-and-mahindra','tata-motors',\n",
    "                     'apollo-hospitals-enterprises','sbi','kotak-mahindra-bank','power-grid-corporation-of-india','axis-bank',\n",
    "                     'ntpc','tech-mahindra','adani-ports','ultratech-cement','nestle','bharat-petroleum']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8194b052",
   "metadata": {},
   "source": [
    "This code is to scrape articles related to specific companies from the Moneycontrol website and save the extracted content into JSON files for each company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for company_name in list_of_companies: \n",
    "    for j in range(10):\n",
    "\n",
    "        main_url = f\"https://www.moneycontrol.com/news/tags/{company_name}.html/page-{j+1}/\"\n",
    "        html_content = requests.get(main_url).content\n",
    "        soup_1 = BeautifulSoup(html_content,'lxml')\n",
    "\n",
    "        list_of_links = []\n",
    "\n",
    "        ref_link_html_collection = soup_1.findAll('li',{'class':'clearfix'})\n",
    "        for ref_link_html_content in ref_link_html_collection:\n",
    "            ref_link_url = ref_link_html_content.h2.a.get('href')\n",
    "            if ref_link_url.find('/news/business/') != -1:\n",
    "               list_of_links.append(ref_link_url)\n",
    "\n",
    "    list_of_content = []\n",
    "    for secondary_url in list_of_links:\n",
    "\n",
    "        secondary_url_html_content = requests.get(secondary_url).content\n",
    "        soup_2 = BeautifulSoup(secondary_url_html_content,\"lxml\")\n",
    "        web_content_each_url = soup_2.findAll('div',{'class':\"content_wrapper arti-flow\"})\n",
    "\n",
    "        paragraph = \"\"\n",
    "        for content in web_content_each_url :\n",
    "            all_p_tags_in_paragraph_html_content = content.findAll('p')\n",
    "\n",
    "            for each_p_tag in all_p_tags_in_paragraph_html_content:\n",
    "                text = each_p_tag.text\n",
    "                paragraph += text\n",
    "            list_of_content.append(paragraph)\n",
    "\n",
    "    with open(f\"{company_name}.json\",'w') as f:\n",
    "        json.dump(list_of_content,f,indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaa46d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "print(len(list_of_companies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
